# Ð Ð°ÑÑ‡ÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸  ðŸ“ˆ

Ð’ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÑÑ€ÐµÐ´Ð½ÑÑ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ° Ð¿Ð¾Ð¿Ð°Ð´Ð°Ð½Ð¸Ñ Ð¸Ð»Ð¸ ÑÑ€ÐµÐ´Ð½ÑÑ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ â€” **Mean Accuracy (mAcc)**. mAcc Ñ€Ð°Ð²Ð½Ð° ÑÑ€ÐµÐ´Ð½ÐµÐ¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¾Ñ†ÐµÐ½Ð¾Ðº Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ° Ð¿Ð¾ Ð²ÑÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐµ:

$$ mAcc = \dfrac{1}{N}\sum_{i=0}^{N-1}\dfrac{TP_i}{TN_i + FN_i} $$

Ð’ ÑÑ‚Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ðµ: `mAcc` â€” ÑÑ€ÐµÐ´Ð½ÑÑ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ, `TP (True Positive)` â€” Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹, `FN (False Negaive)` â€” Ð»Ð¾Ð¶Ð½Ð¾Ð¾Ñ‚Ñ€Ð¸Ñ†Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹, `N` â€” Ñ‡Ð¸ÑÐ»Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð².
Ð’Ñ‹ÑÐ¾ÐºÐ¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ ÑÐ¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ñ Ð·Ð°Ð´Ð°Ñ‡ÐµÐ¹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð² ÑÑ€ÐµÐ´Ð½ÐµÐ¼ Ð´Ð»Ñ Ð²ÑÐµÑ… ÐºÐ»Ð°ÑÑÐ¾Ð². Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð±ÑƒÐ´ÐµÑ‚ Ð¾ÐºÑ€ÑƒÐ³Ð»ÐµÐ½Ð¾ Ð´Ð¾ 5 Ð·Ð½Ð°ÐºÐ° Ð¿Ð¾ÑÐ»Ðµ Ð·Ð°Ð¿ÑÑ‚Ð¾Ð¹. Ð’ ÑÐ»ÑƒÑ‡Ð°Ðµ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ñ Ð² Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³Ðµ Ð±ÑƒÐ´ÐµÑ‚ Ð²Ñ‹ÑˆÐµ Ñƒ Ñ‚Ð¾Ð³Ð¾ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð±Ñ‹Ð»Ð¾ Ñ€Ð°Ð½ÑŒÑˆÐµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð½Ð° Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñƒ.

ÐŸÑÐµÐ²Ð´Ð¾ÐºÐ¾Ð´ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸:
```python
import numpy as np
from sklearn.metrics import confusion_matrix

def mean_class_accuracy(predicts: list[int], labels: list[int]) -> np.ndarray:
    """Calculate mean class accuracy.

    Args:
        predicts (list[int]): Prediction labels for each class.
        labels (list[int]): Ground truth labels.

    Returns:
        np.ndarray: Mean class accuracy.
    """

    conf_matrix = confusion_matrix(y_pred=predicts, y_true=labels)

    cls_cnt = conf_matrix.sum(axis=1) # all labels
    cls_hit = np.diag(conf_matrix) # true positives

    metrics = [hit / cnt if cnt else 0.0 for cnt, hit in zip(cls_cnt, cls_hit)]
    mean_class_acc = np.mean(metrics)

    return mean_class_acc
```